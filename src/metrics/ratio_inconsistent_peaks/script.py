import sys
from collections import defaultdict

import anndata as ad
import numpy as np
import pandas as pd

## VIASH START
# The following code has been auto-generated by Viash.
par = {
    "input_unintegrated": "resources_test/task_cyto_batch_integration/mouse_spleen_flow_cytometry_subset/unintegrated.h5ad",
    "input_integrated_split1": "resources_test/task_cyto_batch_integration/mouse_spleen_flow_cytometry_subset/integrated_split1.h5ad",
    "input_integrated_split2": "resources_test/task_cyto_batch_integration/mouse_spleen_flow_cytometry_subset/integrated_split2.h5ad",
    "output": "resources_test/task_cyto_batch_integration/mouse_spleen_flow_cytometry_subset/score.h5ad",
}
meta = {
    "name": "ratio_inconsistent_peaks",
}

# for local testing only
# import src.metrics.ratio_inconsistent_peaks.helper as metric_helper
# from src.utils.helper_functions import (
#     get_obs_var_for_integrated,
#     remove_unlabelled,
#     subset_markers_tocorrect,
#     subset_nocontrols,
# )

## VIASH END

sys.path.append(meta["resources_dir"])

import helper as metric_helper

# from helper import call_peaks, get_kde_density
from helper_functions import (
    get_obs_var_for_integrated,
    remove_unlabelled,
    subset_markers_tocorrect,
    subset_nocontrols,
)

print("Reading input files", flush=True)
integrated_s1 = ad.read_h5ad(par["input_integrated_split1"])
integrated_s2 = ad.read_h5ad(par["input_integrated_split2"])
unintegrated = ad.read_h5ad(par["input_unintegrated"])

print("Formatting input files", flush=True)
integrated_s1, integrated_s2 = get_obs_var_for_integrated(
    integrated_s1, integrated_s2, unintegrated
)

integrated_s1 = subset_nocontrols(integrated_s1)
integrated_s1 = subset_markers_tocorrect(integrated_s1)
integrated_s1 = subset_nocontrols(integrated_s1)
integrated_s1 = remove_unlabelled(integrated_s1)

integrated_s2 = subset_nocontrols(integrated_s2)
integrated_s2 = subset_markers_tocorrect(integrated_s2)
integrated_s2 = subset_nocontrols(integrated_s2)
integrated_s2 = remove_unlabelled(integrated_s2)

donor_list = integrated_s1.obs["donor"].unique()

print("Compute metric (per cell type)", flush=True)

# case 1 = consistent peaks in unintegrated and also in integrated
# case 3 = consistent peaks in unintegrated but inconsistent in integrated
# not recording case 2 or 4 where unintegrated is inconsistent
n_case1 = 0
n_case3 = 0

# so we can see where each cases comes from
case_details = defaultdict(list)

# for comparison only
persistent_peaks_res = []


for donor in donor_list:
    # for testing only
    # donor = donor_list[0]

    print("Processing donor", donor, flush=True)

    u_view = unintegrated[unintegrated.obs["donor"] == donor]

    # process per split
    s1_view = integrated_s1[integrated_s1.obs["donor"] == donor]
    s2_view = integrated_s2[integrated_s2.obs["donor"] == donor]

    celltype_list = s1_view.obs["cell_type"].unique()

    for celltype in celltype_list:
        # for testing only
        # celltype = celltype_list[0]

        print(f"Processing celltype {celltype}", flush=True)

        u_view_ct = u_view[u_view.obs["cell_type"] == celltype]
        s1_view_ct = s1_view[s1_view.obs["cell_type"] == celltype]
        s2_view_ct = s2_view[s2_view.obs["cell_type"] == celltype]

        if s1_view_ct.shape[0] < 100 or s2_view_ct.shape[0] < 100:
            print(f"Skipping celltype {celltype} and donor {donor}.", flush=True)
            if s1_view_ct.shape[0] < 100:
                print(
                    f"Because n_cells in s1 is {s1_view_ct.shape[0]}, less than 100",
                    flush=True,
                )
            else:
                print(
                    f"Because n_cells in s2 is {s2_view_ct.shape[0]}, less than 100",
                    flush=True,
                )
            # TODO uncomment me when done
            continue

        for marker in s1_view_ct.var.index:
            # for testing only
            # marker = u_view_ct.var.index[0]

            print(f"Processing marker {marker} for celltype {celltype}", flush=True)

            print("--------------------------------", flush=True)
            print("Computing peaks for unintegrated", flush=True)
            # unintegrated for split 1
            u_view_ct_s1 = u_view_ct[u_view_ct.obs["split"] == 1]
            u_view_ct_s2 = u_view_ct[u_view_ct.obs["split"] == 2]

            print("Standardising marker expression", flush=True)
            # standardise marker expression based on pooled mean and sd of
            # unscaled marker expression for unintegrated data for split 1 and 2
            u_s1_unscaled = np.array(u_view_ct_s1[:, marker].layers["preprocessed"])
            u_s2_unscaled = np.array(u_view_ct_s2[:, marker].layers["preprocessed"])

            u_s1_scaled, u_s2_scaled = metric_helper.standardise_marker_expression(
                u_s1_unscaled,
                u_s2_unscaled,
            )
            print("Computing KDE density", flush=True)
            density_dist_u_s1 = metric_helper.get_kde_density(u_s1_scaled)
            density_dist_u_s2 = metric_helper.get_kde_density(u_s2_scaled)

            print("Calling peaks", flush=True)

            peaks_u_s1 = metric_helper.call_peaks(density_dist_u_s1)
            peaks_u_s2 = metric_helper.call_peaks(density_dist_u_s2)

            # use persistent peak only if the peak calling method is too sensitive...
            persistent_peak_count_u_s1 = metric_helper.persistent_peak_count(
                density_dist_u_s1
            )
            persistent_peak_count_u_s2 = metric_helper.persistent_peak_count(
                density_dist_u_s2
            )

            print("--------------------------------", flush=True)

            print("\n", flush=True)

            print("--------------------------------", flush=True)
            print("Computing peaks for integrated", flush=True)

            print("Standardising marker expression", flush=True)
            # standardise marker expression based on pooled mean and sd of
            # unscaled marker expression for unintegrated data for split 1 and 2
            s1_unscaled = np.array(s1_view_ct[:, marker].layers["integrated"])
            s2_unscaled = np.array(s2_view_ct[:, marker].layers["integrated"])

            s1_scaled, s2_scaled = metric_helper.standardise_marker_expression(
                s1_unscaled,
                s2_unscaled,
            )
            print("Computing KDE density", flush=True)
            density_dist_s1 = metric_helper.get_kde_density(s1_scaled)
            density_dist_s2 = metric_helper.get_kde_density(s2_scaled)

            print("Calling peaks", flush=True)

            peaks_s1 = metric_helper.call_peaks(density_dist_s1)
            peaks_s2 = metric_helper.call_peaks(density_dist_s2)

            # use persistent peak only if the peak calling method is too sensitive...
            persistent_peak_count_s1 = metric_helper.persistent_peak_count(
                density_dist_s1
            )
            persistent_peak_count_s2 = metric_helper.persistent_peak_count(
                density_dist_s2
            )

            print("--------------------------------", flush=True)

            print("\n", flush=True)

            print(
                f"Comparing peaks between unintegrated and integrated for {donor}, {celltype}, {marker}",
                flush=True,
            )

            # case 1 or 3 where we have consistent peaks in unintegrated
            if peaks_u_s1 == peaks_u_s2:
                if peaks_s1 != peaks_s2:
                    n_case3 += 1
                    case_details["case3"].append((donor, celltype, marker))
                else:
                    n_case1 += 1
                    case_details["case1"].append((donor, celltype, marker))
            else:
                print(
                    "WARNING! Inconsistent peaks detected in unintegrated data (case 2 or 4). Skipping calculation",
                    flush=True,
                )
                print(
                    f"Number of peaks in unintegrated split 1: {peaks_u_s1}, split 2: {peaks_u_s2}",
                    flush=True,
                )
                case_details["case2or4"].append((donor, celltype, marker))

            # for comparison only
            persistent_peaks_res.append(
                [
                    donor,
                    celltype,
                    marker,
                    peaks_u_s1,
                    peaks_u_s2,
                    persistent_peak_count_u_s1,
                    persistent_peak_count_u_s2,
                    peaks_s1,
                    peaks_s2,
                    persistent_peak_count_s1,
                    persistent_peak_count_s2,
                ]
            )
            print("Done comparing peaks.", flush=True)
            print("\n", flush=True)

print("Done processing all celltypes and donors", flush=True)
print("Calculating ratio", flush=True)

if n_case1 + n_case3 == 0:
    print(
        "Only case 2 or 4 are found!. Cannot calculate metric.",
        flush=True,
    )
    metric_val = np.nan
else:
    metric_val = n_case3 / (n_case1 + n_case3)

persistent_peaks_res = pd.DataFrame(
    persistent_peaks_res,
    columns=[
        "donor",
        "celltype",
        "marker",
        "peaks_u_s1",
        "peaks_u_s2",
        "persistent_peaks_u_s1",
        "persistent_peaks_u_s2",
        "peaks_s1",
        "peaks_s2",
        "persistent_peaks_s1",
        "persistent_peaks_s2",
    ],
)

print("Write output AnnData to file", flush=True)
output = ad.AnnData(
    uns={
        "dataset_id": integrated_s1.uns["dataset_id"],
        "method_id": integrated_s1.uns["method_id"],
        "metric_ids": [meta["name"]],
        "metric_values": [metric_val],
        "n_cases": {
            "case1": n_case1,
            "case3": n_case3,
            "case2or4": len(case_details["case2or4"]),
        },
        "case_details": dict(case_details),
        "peak_calling_results_comparison": persistent_peaks_res,
    }
)
output.write_h5ad(par["output"], compression="gzip")

# print(uns_metric_ids, uns_metric_values)
